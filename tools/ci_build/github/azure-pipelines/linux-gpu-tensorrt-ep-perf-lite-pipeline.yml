parameters:
  - name: CudaVersion
    displayName: CUDA version
    type: string
    default: '12.2'
    values:
      - 12.2
  
  - name: SpecificArtifact
    displayName: Use Specific Artifact
    type: boolean
    default: false

  - name: BuildId
    displayName: Specific Artifact's BuildId
    type: string
    default: '0'

  - name: TrtEPOptions
    displayName: TensorRT EP options
    type: object
    default:
      - trt_max_workspace_size=4294967296
      - trt_engine_cache_enable=True

  - name: CUDAEPOptions
    displayName: CUDA EP options
    type: object
    default: []

variables:
  - template: templates/common-variables.yml
  - name: docker_base_image
    value: onnxruntimebuildcache.azurecr.io/internal/azureml/onnxruntime/build/cuda12_x64_ubi8_gcc12:20250124.1
  - name: linux_trt_version
    value: ${{ variables.linux_trt_version_cuda12 }}
  - name: trtEPOptionsArg
    ${{ if not(eq(length(parameters.TrtEPOptions), 0)) }}:
      value: --trt_ep_options ${{ join(',',parameters.TrtEPOptions) }}
  - name: cudaEPOptionsArg
    ${{ if not(eq(length(parameters.CUDAEPOptions), 0)) }}:
      value: --cuda_ep_options ${{ join(',',parameters.CUDAEPOptions) }}
  - name: optional_arguments
    value: -a "-a -z -g $(optimizeGraph) -b $(bindInputs) $(trtEPOptionsArg) $(cudaEPOptionsArg)"
  - name: Repository
    value: onnxruntime_ep_perf_lite_build

stages:
- stage: Linux_Build
  jobs:
  - job: Linux_Build
    timeoutInMinutes: 180
    variables:
      skipComponentGovernanceDetection: true
      ALLOW_RELEASED_ONNX_OPSET_ONLY: '1'
      ORT_CACHE_DIR: '$(Agent.TempDirectory)/ort/ccache'
      TODAY: $[format('{0:dd}{0:MM}{0:yyyy}', pipeline.startTime)]
    workspace:
      clean: all
    pool: onnxruntime-tensorrt-linuxbuild-T4
    steps:

    - checkout: self
      clean: true
      submodules: none

    - template: templates/get-docker-image-steps.yml
      parameters:
        Dockerfile: tools/ci_build/github/linux/docker/Dockerfile.manylinux2_28_cuda
        Context: tools/ci_build/github/linux/docker
        DockerBuildArgs: "
        --network=host
        --build-arg BASEIMAGE=${{ variables.docker_base_image }}
        --build-arg TRT_VERSION=${{ variables.linux_trt_version }}
        --build-arg BUILD_UID=$( id -u )
        "
        Repository: ${{ variables.Repository }}

    - template: templates/linux-build-step-with-cache.yml
      parameters:
        WithCache: true
        Today: $(TODAY)
        AdditionalKey: gpu_tensorrt
        CacheDir: '$(ORT_CACHE_DIR)'
        BuildStep:
          - task: CmdLine@2
            inputs:
              script: |
                docker run -e SYSTEM_COLLECTIONURI --gpus all --rm \
                    --volume /data/onnx:/data/onnx:ro \
                    --volume $(Build.SourcesDirectory):/onnxruntime_src \
                    --volume $(Build.BinariesDirectory):/build \
                    --volume /data/models:/build/models:ro \
                    --volume $HOME/.onnx:/home/onnxruntimedev/.onnx \
                    --volume $(ORT_CACHE_DIR):/cache \
                    -e ALLOW_RELEASED_ONNX_OPSET_ONLY=0 \
                    -e NIGHTLY_BUILD \
                    -e BUILD_BUILDNUMBER \
                    -e CCACHE_DIR=/cache -w /onnxruntime_src \
                    ${{ variables.Repository }} tools/ci_build/github/linux/build_tensorrt_ci.sh
              workingDirectory: $(Build.SourcesDirectory)
              displayName: Build Onnxruntime with TensorRT EP

    - script: $(Build.SourcesDirectory)/tools/ci_build/github/linux/delete_unused_files_before_upload.sh

    - task: PublishPipelineArtifact@0
      displayName: 'Publish Pipeline Artifact'
      inputs:
        artifactName: 'onnxruntime-linux-x64-tensorrt'
        targetPath: '$(Build.BinariesDirectory)/Release'
    - template: templates/explicitly-defined-final-tasks.yml

- stage: Linux_Test
  dependsOn:
    - Linux_Build
  jobs:
  - job: Linux_Test
    timeoutInMinutes: 180
    variables:
      skipComponentGovernanceDetection: true
    workspace:
      clean: all
    pool: Onnxruntime-Linux-GPU-A100-WUS3
    steps:
    - checkout: self
      clean: true
      submodules: none

    - template: templates/flex-downloadPipelineArtifact.yml
      parameters:
        ArtifactName: 'onnxruntime-linux-x64-tensorrt'
        StepName: 'Download Pipeline Artifact - Linux Build'
        TargetPath: '$(Build.BinariesDirectory)/Release'
        SpecificArtifact: ${{ parameters.SpecificArtifact }}
        BuildId: ${{ parameters.BuildId }}

    - template: templates/get-docker-image-steps.yml
      parameters:
        Dockerfile: tools/ci_build/github/linux/docker/Dockerfile.manylinux2_28_cuda
        Context: tools/ci_build/github/linux/docker
        DockerBuildArgs: "--build-arg BASEIMAGE=$(docker_base_image) --build-arg BUILD_UID=$( id -u )"
        Repository: ${{ variables.Repository }}

    - task: CmdLine@2
      inputs:
        script: |
          set -e -x
          mkdir -p $HOME/.onnx
          docker run --gpus all --rm \
            --volume $(Build.SourcesDirectory):/onnxruntime_src \
            --volume $(Build.BinariesDirectory)/Release:/build/Release \
            --volume /data/models:/build/models:ro \
            --volume $HOME/.onnx:/home/onnxruntimedev/.onnx \
            --volume /data/onnx:/data/onnx \
            -e NVIDIA_TF32_OVERRIDE=0 \
            ${{ variables.Repository }} \
            /bin/bash -c '
              set -e
              nvidia-smi; \
              /sbin/ldconfig -N -v $(sed "s/:/ /" <<< $LD_LIBRARY_PATH) 2>/dev/null | grep -E "libcudart.so|libcudnn.so|libnvinfer.so"; \
              cat /usr/local/cuda/include/cuda.h | grep -m1 CUDA_VERSION; \
              cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -m1 -A 2; \
              export PATH=/opt/python/cp312-cp312/bin:$PATH; \
              which python3; \
              df -h; \
              python3 -m pip install /build/Release/dist/*.whl; \
              python3 -u -c "from onnxruntime.capi._pybind_state import (OrtDevice as C_OrtDevice) ; \
                        ort_device = C_OrtDevice(C_OrtDevice.cuda(), C_OrtDevice.default_memory(), 0); \
                        print(ort_device); print(ort_device.device_type(), C_OrtDevice.cuda()); \
                        assert(ort_device.device_type()==1); assert(C_OrtDevice.cuda()==1);" \
            '
      displayName: 'Check GPU'

    - task: CmdLine@2
      inputs:
        script: |
          set -e -x
          mkdir -p $HOME/.onnx
          docker run --gpus all --shm-size=1g --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm \
            --volume $(Build.SourcesDirectory):/onnxruntime_src \
            --volume $(Build.BinariesDirectory)/Release:/build/Release \
            --volume /data/models:/build/models:ro \
            --volume $HOME/.onnx:/home/onnxruntimedev/.onnx \
            --volume /data/onnx:/data/onnx \
            --volume $(Build.SourcesDirectory)/onnxruntime/python/tools/tensorrt/perf/mem_test:/mem_test \
            --volume /data/ep-perf-models:/data/ep-perf-models \
            -e NVIDIA_TF32_OVERRIDE=0 \
            $(Repository) \
            /bin/bash -c '
              set -ex; \
              cd /onnxruntime_src/onnxruntime/python/tools/tensorrt/perf; \
              ./perf.sh -d /onnxruntime_src/onnxruntime/python/tools/tensorrt/perf \
                    -o onnx-zoo-models \
                    -m $(onnx-zoo-models) \
                    -b true \
                    -e "$(epList)" \
                    $(optional_arguments) ; \
            '
      displayName: 'Run onnx-zoo-model inference test'

    # Prepare and Publish Artifacts
    - script: |
          mkdir -p $(Build.SourcesDirectory)/Artifact
      displayName: 'Prepare Artifacts Directory'
      workingDirectory: $(Build.SourcesDirectory)/onnxruntime/python/tools/tensorrt/perf/
      condition: always()

    - script: |
          cp -r $(Build.SourcesDirectory)/onnxruntime/python/tools/tensorrt/perf/result $(Build.SourcesDirectory)/Artifact
      displayName: 'Copy Artifacts'
      workingDirectory: $(Build.SourcesDirectory)/onnxruntime/python/tools/tensorrt/perf/
      condition: always()

    - task: PublishBuildArtifacts@1
      inputs:
        pathtoPublish: '$(Build.SourcesDirectory)/Artifact'
        artifactName: 'result-$(Build.BuildNumber)'

    - template: templates/clean-agent-build-directory-step.yml
